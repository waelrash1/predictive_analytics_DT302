{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyM9dtjm8QwSp1f8CPzC1hMx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/waelrash1/predictive_analytics_DT302/blob/main/SVC_Lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Introduction to SVM\n",
        "1- Explain the concept of Support Vector Machines (SVM). SVM is a supervised machine learning algorithm which can be used for both classification and regression challenges. In this notebook, we focus on classification.\n",
        "\n"
      ],
      "metadata": {
        "id": "LoZ-9j9vaS73"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2- Importing Required Libraries"
      ],
      "metadata": {
        "id": "6kDEqnOBaZug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ],
      "metadata": {
        "id": "5-PAjlYbanJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Loading the Breast Cancer Dataset\n"
      ],
      "metadata": {
        "id": "fYodHOQgaxf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "cancer = datasets.load_breast_cancer()\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(np.c_[cancer['data'], cancer['target']],\n",
        "                  columns=np.append(cancer['feature_names'], ['target']))"
      ],
      "metadata": {
        "id": "vl9cg6-ua32m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Data Preprocessing"
      ],
      "metadata": {
        "id": "_fXRyqgfbCMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first few rows of the DataFrame\n",
        "print(df.head())\n",
        "\n",
        "# Dataset dimensions\n",
        "print(df.shape)"
      ],
      "metadata": {
        "id": "zm9d6i2ibMBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting the Data\n"
      ],
      "metadata": {
        "id": "Dikupc0PbU_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Splitting the dataset into a training set and a testing set\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "reU8h_nsbTwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create SVC Linear  Kernal pipeline\n"
      ],
      "metadata": {
        "id": "7rF0qTaybZwZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('svm', SVC(kernel='linear'))\n",
        "])\n"
      ],
      "metadata": {
        "id": "NT8C2e38bdpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Train the pipeline"
      ],
      "metadata": {
        "id": "BzWoODkIbhJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a SVM Classifier\n",
        "# Training the pipeline\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n"
      ],
      "metadata": {
        "id": "jX3eJd5Ybjb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Model Evaluation\n",
        "Evaluating the Model"
      ],
      "metadata": {
        "id": "PCd2eiX5b0oK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Making predictions\n",
        "y_pred = pipeline.predict(X_test)"
      ],
      "metadata": {
        "id": "SaCMJXt2bxnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print(conf_matrix)"
      ],
      "metadata": {
        "id": "Hj2FmW2ab4wv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification Report\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "print(class_report)"
      ],
      "metadata": {
        "id": "WxKs28YAb8vr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualizing the Confusion Matrix\n",
        "\n",
        "sns.heatmap(conf_matrix, annot=True)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Truth')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2PVcowFgb_Md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Polynomial SVC Kernel"
      ],
      "metadata": {
        "id": "2rfFGnogfyhG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Polynomial kernel\n",
        "poly_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('svm_poly', SVC(kernel='poly', degree=1))  # degree can be adjusted\n",
        "])\n",
        "poly_pipeline.fit(X_train, y_train)\n",
        "y_pred_poly = poly_pipeline.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "print(confusion_matrix(y_test, y_pred_poly))\n",
        "print(classification_report(y_test, y_pred_poly))\n"
      ],
      "metadata": {
        "id": "s79VRbgwfyC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RBF Kernel"
      ],
      "metadata": {
        "id": "HovKVViogRuu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RBF kernel\n",
        "rbf_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('svm_rbf', SVC(kernel='rbf'))\n",
        "])\n",
        "rbf_pipeline.fit(X_train, y_train)\n",
        "y_pred_rbf = rbf_pipeline.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "print(confusion_matrix(y_test, y_pred_rbf))\n",
        "print(classification_report(y_test, y_pred_rbf))\n"
      ],
      "metadata": {
        "id": "7n8O2FmRgK0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SVC Parameter Tuning\n",
        "Start by explaining the importance of parameter tuning in machine learning models, particularly for SVM. Discuss the parameters:\n",
        "\n",
        "* C: Regularization parameter. The strength of the regularization is inversely proportional to C. It helps to avoid overfitting.\n",
        "* Kernel: Specifies the kernel type to be used in the algorithm.\n",
        "* Gamma: Kernel coefficient for 'rbf', 'poly', and 'sigmoid'. It defines how far the influence of a single training example reaches.\n",
        "\n",
        "2. Setting Up Grid Search\n",
        "First, import the necessary module for Grid Search:"
      ],
      "metadata": {
        "id": "bwXCpGSZgsYb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "# Now, set up the parameter grid to test:\n",
        "\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],  # A range of values for C\n",
        "    'gamma': [1, 0.1, 0.01, 0.001],  # A range of values for gamma\n",
        "    'kernel': ['linear','rbf', 'poly', 'sigmoid']  # Different kernel types\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "id": "1P8vWI-8goKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Applying Grid Search\n",
        "\n",
        "Create a GridSearchCV object and fit it to the training data:\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sI0B53PyiDmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=1)\n",
        "grid.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "adwO5RawiHdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Evaluating the Best Model\n",
        "\n",
        "After fitting, we can check the best parameter combination found by Grid Search:\n",
        "\n"
      ],
      "metadata": {
        "id": "43Kt9iAWiQXE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"Best Parameters Found: \", grid.best_params_)\n"
      ],
      "metadata": {
        "id": "WkRe-1J_iwaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Use the best estimator to make predictions:\n",
        "grid_predictions = grid.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "print(confusion_matrix(y_test, grid_predictions))\n",
        "print(classification_report(y_test, grid_predictions))"
      ],
      "metadata": {
        "id": "0Bj8re2EiUAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression\n",
        "\n",
        "Importing Logistic Regression\n",
        "You've already imported necessary libraries. Now, import Logistic Regression from scikit-learn.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hzxV_Tmjevsp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "# Creating a Pipeline for Logistic Regression\n",
        "\n",
        "# Creating a pipeline for logistic regression\n",
        "logistic_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('logistic', LogisticRegression())\n",
        "])\n",
        "\n",
        "# Training the logistic regression pipeline\n",
        "logistic_pipeline.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "p_1PNHN9e5oQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making predictions\n",
        "y_pred_logistic = logistic_pipeline.predict(X_test)\n",
        "\n",
        "# Evaluating Logistic Regression Model\n",
        "\n",
        "# Confusion Matrix for Logistic Regression\n",
        "conf_matrix_logistic = confusion_matrix(y_test, y_pred_logistic)\n",
        "print(conf_matrix_logistic)\n",
        "\n",
        "# Classification Report for Logistic Regression\n",
        "class_report_logistic = classification_report(y_test, y_pred_logistic)\n",
        "print(class_report_logistic)\n",
        "# Visualizing the Confusion Matrix for Logistic Regression\n",
        "\n",
        "sns.heatmap(conf_matrix_logistic, annot=True)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Truth')\n",
        "plt.title('Confusion Matrix for Logistic Regression')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8myW7FIhfDff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "7. Conclusion\n",
        "Summarize the performance of the model and discuss the results. You can discuss how changing kernel types and tuning parameters can affect the model's performance.\n",
        "\n",
        "8. Additional Exercises and Resources\n",
        "Encourage students to try different kernels ('rbf', 'poly', etc.) and play with the C and gamma parameters. Provide links to further reading."
      ],
      "metadata": {
        "id": "uyQPFRofcWBV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xBvq_xmJaQgs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MNIST Dataset"
      ],
      "metadata": {
        "id": "2kPvVw47j416"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets, svm, metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n"
      ],
      "metadata": {
        "id": "02ub1rB8j87q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "digits = datasets.load_digits()\n",
        "\n",
        "# Displaying the shape of data and target\n",
        "print(\"Image Data Shape: \", digits.data.shape)\n",
        "print(\"Label Data Shape: \", digits.target.shape)\n",
        "\n",
        "# Displaying the first few images and labels\n",
        "fig, axes = plt.subplots(1, 10, figsize=(10, 3))\n",
        "for ax, image, label in zip(axes, digits.images, digits.target):\n",
        "    ax.set_axis_off()\n",
        "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
        "    ax.set_title('Training: %i' % label)\n"
      ],
      "metadata": {
        "id": "3rQVajpNj-lZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into 70% train and 30% test subsets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    digits.data, digits.target, test_size=0.3, shuffle=False)\n"
      ],
      "metadata": {
        "id": "9ZB9bF-Ck4qq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a classifier: a support vector classifier\n",
        "svm_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('svm', svm.SVC(gamma=0.001))\n",
        "])\n",
        "\n",
        "# Fit to the training data\n",
        "svm_pipeline.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "gZmA9luXk9HD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict the value of the digit on the test subset\n",
        "predicted = svm_pipeline.predict(X_test)\n"
      ],
      "metadata": {
        "id": "hvLl7krYlBwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion matrix\n",
        "print(f\"Confusion matrix:\\n{metrics.confusion_matrix(y_test, predicted)}\")\n",
        "\n",
        "# Classification report\n",
        "print(f\"Classification report for classifier {svm_pipeline}:\\n\"\n",
        "      f\"{metrics.classification_report(y_test, predicted)}\")\n"
      ],
      "metadata": {
        "id": "JcGZIIEUlER2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xTtPoV4Kj69Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grid Search"
      ],
      "metadata": {
        "id": "4IR3TTA4lsTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "# Now, set up the parameter grid to test:\n",
        "\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],  # A range of values for C\n",
        "    'gamma': [1, 0.1, 0.01, 0.001],  # A range of values for gamma\n",
        "    'kernel': ['linear','rbf', 'poly', 'sigmoid']  # Different kernel types\n",
        "}\n"
      ],
      "metadata": {
        "id": "BElcOsRylxmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=1)\n",
        "grid.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "bdubWjy_l4hy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Best Parameters Found: \", grid.best_params_)\n"
      ],
      "metadata": {
        "id": "dyhbaNgbl8zf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Use the best estimator to make predictions:\n",
        "grid_predictions = grid.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "print(confusion_matrix(y_test, grid_predictions))\n",
        "print(classification_report(y_test, grid_predictions))"
      ],
      "metadata": {
        "id": "GgR4Y35hmAxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experimenting with Banknote Authentication Dataset\n",
        "Begin by introducing the dataset. The Banknote Authentication Dataset contains images of genuine and forged banknote-like specimens. Features are extracted from these images, such as variance, skewness, curtosis of the wavelet-transformed image, and entropy.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YQL39USKm76_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Required Libraries and Dataset\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.pipeline import Pipeline\n",
        "import urllib.request"
      ],
      "metadata": {
        "id": "ybFMNzIznHs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the Banknote Authentication Dataset\n",
        "# The dataset can be downloaded from the UCI Machine Learning Repository. Here's how to do it:\n",
        "\n",
        "\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt\"\n",
        "urllib.request.urlretrieve(url, \"data_banknote_authentication.csv\")"
      ],
      "metadata": {
        "id": "XAHUyx94nQoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the dataset\n",
        "df = pd.read_csv(\"data_banknote_authentication.csv\", header=None)\n",
        "df.columns = [\"Variance\", \"Skewness\", \"Curtosis\", \"Entropy\", \"Class\"]\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "1XCqCJVBnbC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preprocessing\n",
        "#Exploratory Data Analysis\n",
        "\n",
        "# Basic stats\n",
        "print(df.describe())\n",
        "\n",
        "# Checking for null values\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Class distribution\n",
        "sns.countplot(df['Class'])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jBWkR3B0nlH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Split the data into features and target label\n",
        "X = df.drop('Class', axis=1)\n",
        "y = df['Class']\n",
        "\n",
        "# Splitting the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "# Building and Training the SVM Model\n",
        "\n",
        "# Creating a SVM Classifier with a radial basis function (rbf) kernel\n",
        "svm_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('svm', SVC(kernel='linear'))\n",
        "])\n",
        "\n",
        "# Train the model using the training sets\n",
        "svm_pipeline.fit(X_train, y_train)\n",
        "# Model Evaluation\n",
        "# Making Predictions and Evaluating the Model\n",
        "\n",
        "# Predicting the Test set results\n",
        "y_pred = svm_pipeline.predict(X_test)\n",
        "\n",
        "# Confusion Matrix\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Classification Report\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "nsN4uG-ln3oS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Experimenting with Different Kernels\n",
        "Encourage experimentation with different kernels ('linear', 'poly', 'sigmoid') and parameters (C, gamma) to observe their impact on the model's performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "Faj31GBmoJ97"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Fashion MNIST Dataset\n",
        "Start by introducing the dataset. The Fashion MNIST dataset is a collection of 70,000 grayscale images of 10 fashion categories, including shirts, dresses, shoes, etc. Each image is 28x28 pixels. This dataset is often used as a more challenging replacement for the classic MNIST dataset.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YSJWuD3WozcW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Required Libraries and Dataset\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import svm, metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "import tensorflow as tf\n",
        "\n"
      ],
      "metadata": {
        "id": "FjsEolVuo8io"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the Fashion MNIST Dataset\n",
        "# Fashion MNIST can be easily loaded via TensorFlow or Keras:\n",
        "\n",
        "\n",
        "# Load the Fashion MNIST dataset\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "\n"
      ],
      "metadata": {
        "id": "4T_Btk_4pGGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the Fashion MNIST dataset\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Define class names for Fashion MNIST\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "# Plotting a few samples from the dataset\n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(X_train[i], cmap=plt.cm.binary)\n",
        "    plt.xlabel(class_names[y_train[i]])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "AnKvA3-xq2K0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Normalize the data\n",
        "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
        "\n",
        "# Reshape the data to fit the SVM input requirements\n",
        "X_train = X_train.reshape(X_train.shape[0], -1)\n",
        "X_test = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "print(\"Training Set Shape:\", X_train.shape)\n",
        "print(\"Test Set Shape:\", X_test.shape)"
      ],
      "metadata": {
        "id": "JP-SEXBhq2-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preprocessing\n",
        "# Since we have already normalized and reshaped the data, no further preprocessing is needed.\n",
        "\n",
        "# Building and Training the SVM Model\n",
        "# Given the size of the dataset, consider using a subset of the training data for faster processing, or use a linear kernel for quicker execution.\n",
        "\n",
        "\n",
        "\n",
        "# Using a linear kernel for quicker execution\n",
        "svm_model = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('svm', svm.SVC(kernel='rbf'))\n",
        "])\n",
        "\n"
      ],
      "metadata": {
        "id": "qml4wlI9pS0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model (consider using a smaller subset of data for faster processing)\n",
        "svm_model.fit(X_train[:10000], y_train[:10000]) # Using first 10000 samples for training\n",
        "# Model Evaluation\n",
        "\n",
        "# Making predictions\n",
        "y_pred = svm_model.predict(X_test[:1000]) # Using first 1000 samples for testing\n",
        "\n",
        "# Confusion Matrix\n",
        "print(metrics.confusion_matrix(y_test[:1000], y_pred))\n",
        "\n",
        "# Classification Report\n",
        "print(metrics.classification_report(y_test[:1000], y_pred))"
      ],
      "metadata": {
        "id": "qJ454YSYpdDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Visualizing the Predictions\n",
        "#Visualizing predictions can help in understanding where the model performs well or poorly.\n",
        "\n",
        "fig, axes = plt.subplots(3, 3, figsize=(10, 10))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(X_test[i].reshape(28, 28), cmap='gray')\n",
        "    ax.set_title(f\"True: {y_test[i]}, Predicted: {y_pred[i]}\")\n",
        "    ax.set_axis_off()"
      ],
      "metadata": {
        "id": "5Bt7HpwDrWlg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}