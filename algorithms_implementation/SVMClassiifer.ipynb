{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNc9zzS8ygUNFqqLPZZ2Lcb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/waelrash1/predictive_analytics_DT302/blob/main/algorithms_implementation/SVMClassiifer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Support Vector Machine (SVM)**\n",
        "Algorithm is derived and implemented, let's first start by explaining the key concepts and the mathematical derivations. Then, we'll map those equations to Python code.\n",
        "\n",
        "### 1. **SVM Problem Formulation**\n",
        "\n",
        "The **SVM** seeks to find the optimal hyperplane that separates two classes with the maximum margin. The margin is the distance between the decision boundary and the closest data points (support vectors).\n",
        "\n",
        "#### Objective:\n",
        "Given a dataset of $n$ samples $(x_i, y_i)$, where $x_i$ is the input feature vector and $y_i \\in \\{-1, 1\\}$ is the label, we want to find a hyperplane:\n",
        "\n",
        "$\n",
        "f(x) = w^T x - b\n",
        "$\n",
        "\n",
        "that maximizes the margin between the two classes. The hyperplane should satisfy the condition:\n",
        "\n",
        "$\n",
        "y_i (w^T x_i - b) \\geq 1 \\quad \\forall i\n",
        "$\n",
        "\n",
        "### 2. **Primal Optimization Problem**\n",
        "\n",
        "The objective of SVM is to minimize the following cost function (with regularization) and enforce that the points are classified correctly with a margin:\n",
        "\n",
        "$\n",
        "\\min_{w, b} \\frac{1}{2} ||w||^2 \\quad \\text{subject to} \\quad y_i (w^T x_i - b) \\geq 1, \\quad \\forall i\n",
        "$\n",
        "\n",
        "Here, $ \\frac{1}{2} ||w||^2 $ ensures that we are maximizing the margin. However, this is a **constrained optimization problem**.\n",
        "\n",
        "### 3. **Lagrange Multipliers (Converting Constrained to Unconstrained)**\n",
        "\n",
        "To handle the constraints $ y_i (w^T x_i - b) \\geq 1 $, we introduce **Lagrange multipliers** $ \\alpha_i \\geq 0 $, and the optimization problem becomes:\n",
        "\n",
        "$\n",
        "L(w, b, \\alpha) = \\frac{1}{2} ||w||^2 - \\sum_{i=1}^{n} \\alpha_i [y_i (w^T x_i - b) - 1]\n",
        "$\n",
        "\n",
        "Where:\n",
        "- $ L(w, b, \\alpha) $ is the Lagrangian.\n",
        "- $ \\alpha_i $ are the Lagrange multipliers corresponding to each constraint.\n",
        "\n",
        "The **dual problem** (obtained by minimizing with respect to $ w $ and $ b $) becomes:\n",
        "\n",
        "$\n",
        "\\min_w \\max_{\\alpha \\geq 0} \\left( \\frac{1}{2} ||w||^2 - \\sum_{i=1}^{n} \\alpha_i (y_i (w^T x_i - b) - 1) \\right)\n",
        "$\n",
        "\n",
        "### 4. **Hinge Loss (Soft Margin SVM)**\n",
        "\n",
        "In practice, it is hard to perfectly separate all the data points, so we introduce a **slack variable** to allow some misclassification. This gives us the **hinge loss function**:\n",
        "\n",
        "$\n",
        "L(w, b) = \\frac{1}{2} ||w||^2 + C \\sum_{i=1}^{n} \\max(0, 1 - y_i (w^T x_i - b))\n",
        "$\n",
        "\n",
        "Where:\n",
        "- $ C $ controls the tradeoff between maximizing the margin and minimizing the misclassification error.\n",
        "\n",
        "### 5. **Gradient Descent Updates**\n",
        "\n",
        "The SVM problem can be solved using **gradient descent**. We need to compute the gradients of the loss function with respect to the weights $w$ and bias $b$:\n",
        "\n",
        "1. **Gradient w.r.t. weights $w$**:\n",
        "   - If $ y_i (w^T x_i - b) \\geq 1 $, the point is correctly classified, so the gradient is:\n",
        "\n",
        "   $\n",
        "   \\frac{\\partial L}{\\partial w} = \\lambda w\n",
        "   $\n",
        "\n",
        "   - If $ y_i (w^T x_i - b) < 1 $ (misclassified), the gradient becomes:\n",
        "\n",
        "   $\n",
        "   \\frac{\\partial L}{\\partial w} = \\lambda w - y_i x_i\n",
        "   $\n",
        "\n",
        "2. **Gradient w.r.t. bias $b$**:\n",
        "   - If $ y_i (w^T x_i - b) \\geq 1 $, the point is correctly classified, so:\n",
        "\n",
        "   $\n",
        "   \\frac{\\partial L}{\\partial b} = 0\n",
        "   $\n",
        "\n",
        "   - If $ y_i (w^T x_i - b) < 1 $, the gradient is:\n",
        "\n",
        "   $\n",
        "   \\frac{\\partial L}{\\partial b} = - y_i\n",
        "   $\n",
        "\n",
        "### 6. **SVM Implementation from Scratch in Python**\n",
        "\n",
        "Now let's combine these concepts and build the SVM classifier step by step.\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# SVM Classifier\n",
        "class SVM:\n",
        "    def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000):\n",
        "        self.learning_rate = learning_rate  # Step size for gradient descent\n",
        "        self.lambda_param = lambda_param    # Regularization parameter (C)\n",
        "        self.n_iters = n_iters              # Number of iterations\n",
        "        self.w = None                       # Weight vector\n",
        "        self.b = None                       # Bias term\n",
        "\n",
        "    # Function to train the SVM model\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        \n",
        "        # Initialize weights and bias to zero\n",
        "        self.w = np.zeros(n_features)\n",
        "        self.b = 0\n",
        "        \n",
        "        # Convert class labels from {0, 1} to {-1, 1}\n",
        "        y_ = np.where(y <= 0, -1, 1)\n",
        "        \n",
        "        # Gradient descent for n_iters iterations\n",
        "        for _ in range(self.n_iters):\n",
        "            for idx, x_i in enumerate(X):\n",
        "                # Condition for correctly classified points\n",
        "                condition = y_[idx] * (np.dot(x_i, self.w) - self.b) >= 1\n",
        "                if condition:\n",
        "                    # Gradient update for correctly classified points\n",
        "                    self.w -= self.learning_rate * (2 * self.lambda_param * self.w)\n",
        "                else:\n",
        "                    # Gradient update for misclassified points (hinge loss)\n",
        "                    self.w -= self.learning_rate * (2 * self.lambda_param * self.w - np.dot(x_i, y_[idx]))\n",
        "                    self.b -= self.learning_rate * y_[idx]\n",
        "\n",
        "    # Function to make predictions\n",
        "    def predict(self, X):\n",
        "        # Predict the class label based on the sign of the decision boundary\n",
        "        return np.sign(np.dot(X, self.w) - self.b)\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    from sklearn.datasets import make_blobs\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    # Generate synthetic data\n",
        "    X, y = make_blobs(n_samples=100, n_features=2, centers=2, random_state=123)\n",
        "    \n",
        "    # Convert the labels to {-1, 1} for SVM\n",
        "    y = np.where(y == 0, -1, 1)\n",
        "    \n",
        "    # Initialize and train the SVM model\n",
        "    clf = SVM(learning_rate=0.001, lambda_param=0.01, n_iters=1000)\n",
        "    clf.fit(X, y)\n",
        "    \n",
        "    # Make predictions\n",
        "    predictions = clf.predict(X)\n",
        "    \n",
        "    # Calculate accuracy\n",
        "    accuracy = np.mean(predictions == y)\n",
        "    print(f\"SVM Classification accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "    # Plot decision boundary\n",
        "    def plot_decision_boundary(X, y, clf):\n",
        "        plt.scatter(X[:, 0], X[:, 1], marker='o', c=y, s=25, cmap=plt.cm.coolwarm)\n",
        "        ax = plt.gca()\n",
        "        xlim = ax.get_xlim()\n",
        "        ylim = ax.get_ylim()\n",
        "\n",
        "        # Create grid to evaluate model\n",
        "        xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 50), np.linspace(ylim[0], ylim[1], 50))\n",
        "        xy = np.vstack([xx.ravel(), yy.ravel()]).T\n",
        "        Z = clf.predict(xy).reshape(xx.shape)\n",
        "\n",
        "        # Plot decision boundary and margins\n",
        "        ax.contour(xx, yy, Z, colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])\n",
        "        plt.show()\n",
        "\n",
        "    plot_decision_boundary(X, y, clf)\n",
        "```\n",
        "\n",
        "### Key Parts of the Code:\n",
        "\n",
        "- **Initialization**: The weights `w` and bias `b` are initialized to zero. The learning rate controls how much we adjust the weights after each step.\n",
        "  \n",
        "- **Training (Gradient Descent)**: The SVM is trained using gradient descent. For each sample, we check if it satisfies the condition $ y_i (w^T x_i - b) \\geq 1 $. If the point is correctly classified, we only update the weights using the regularization term. Otherwise, we update both weights and bias using the hinge loss.\n",
        "\n",
        "- **Prediction**: The model makes predictions based on the sign of the dot product between the weights and input features.\n",
        "\n",
        "- **Visualization**: The decision boundary is plotted to show the separating hyperplane, and the margins are indicated by dashed lines.\n",
        "\n",
        "### Summary:\n",
        "\n",
        "1. **Formulation**: We convert the constrained SVM optimization problem into an unconstrained problem using Lagrange multipliers and hinge loss.\n",
        "2. **Gradient Descent**: We derive gradients for weights and bias and update them iteratively.\n",
        "3"
      ],
      "metadata": {
        "id": "UAYfzIGmolZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# SVM Classifier\n",
        "class SVM:\n",
        "    def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000):\n",
        "        self.learning_rate = learning_rate  # Step size for gradient descent\n",
        "        self.lambda_param = lambda_param    # Regularization parameter (C)\n",
        "        self.n_iters = n_iters              # Number of iterations\n",
        "        self.w = None                       # Weight vector\n",
        "        self.b = None                       # Bias term\n",
        "\n",
        "    # Function to train the SVM model\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Initialize weights and bias to zero\n",
        "        self.w = np.zeros(n_features)\n",
        "        self.b = 0\n",
        "\n",
        "        # Convert class labels from {0, 1} to {-1, 1}\n",
        "        y_ = np.where(y <= 0, -1, 1)\n",
        "\n",
        "        # Gradient descent for n_iters iterations\n",
        "        for _ in range(self.n_iters):\n",
        "            for idx, x_i in enumerate(X):\n",
        "                # Condition for correctly classified points\n",
        "                condition = y_[idx] * (np.dot(x_i, self.w) - self.b) >= 1\n",
        "                if condition:\n",
        "                    # Gradient update for correctly classified points\n",
        "                    self.w -= self.learning_rate * (2 * self.lambda_param * self.w)\n",
        "                else:\n",
        "                    # Gradient update for misclassified points (hinge loss)\n",
        "                    self.w -= self.learning_rate * (2 * self.lambda_param * self.w - np.dot(x_i, y_[idx]))\n",
        "                    self.b -= self.learning_rate * y_[idx]\n",
        "\n",
        "    # Function to make predictions\n",
        "    def predict(self, X):\n",
        "        # Predict the class label based on the sign of the decision boundary\n",
        "        return np.sign(np.dot(X, self.w) - self.b)\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    from sklearn.datasets import make_blobs\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    # Generate synthetic data\n",
        "    X, y = make_blobs(n_samples=100, n_features=2, centers=2, random_state=123)\n",
        "\n",
        "    # Convert the labels to {-1, 1} for SVM\n",
        "    y = np.where(y == 0, -1, 1)\n",
        "\n",
        "    # Initialize and train the SVM model\n",
        "    clf = SVM(learning_rate=0.001, lambda_param=0.01, n_iters=1000)\n",
        "    clf.fit(X, y)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = clf.predict(X)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = np.mean(predictions == y)\n",
        "    print(f\"SVM Classification accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "    # Plot decision boundary\n",
        "    def plot_decision_boundary(X, y, clf):\n",
        "        plt.scatter(X[:, 0], X[:, 1], marker='o', c=y, s=25, cmap=plt.cm.coolwarm)\n",
        "        ax = plt.gca()\n",
        "        xlim = ax.get_xlim()\n",
        "        ylim = ax.get_ylim()\n",
        "\n",
        "        # Create grid to evaluate model\n",
        "        xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 50), np.linspace(ylim[0], ylim[1], 50))\n",
        "        xy = np.vstack([xx.ravel(), yy.ravel()]).T\n",
        "        Z = clf.predict(xy).reshape(xx.shape)\n",
        "\n",
        "        # Plot decision boundary and margins\n",
        "        ax.contour(xx, yy, Z, colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])\n",
        "        plt.show()\n",
        "\n",
        "    plot_decision_boundary(X, y, clf)"
      ],
      "metadata": {
        "id": "1ww6RZ7Jphhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apologies for the cutoff in my previous message.\n",
        "\n",
        "### 7. **Example on a Dataset**\n",
        "\n",
        "Let's apply our SVM implementation to a real dataset. We'll use the **Breast Cancer Wisconsin** dataset from `sklearn.datasets`, which is a binary classification problem.\n",
        "\n",
        "#### **Dataset Description:**\n",
        "\n",
        "- The dataset contains features computed from a digitized image of a fine needle aspirate (FNA) of a breast mass.\n",
        "- The target variable is binary: malignant (1) or benign (0).\n",
        "\n",
        "#### **Steps:**\n",
        "\n",
        "1. **Load the Dataset.**\n",
        "2. **Preprocess the Data.**\n",
        "3. **Split into Training and Testing Sets.**\n",
        "4. **Train the SVM Model.**\n",
        "5. **Evaluate the Model.**\n",
        "\n",
        "#### **Code Implementation:**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Convert labels from {0,1} to {-1,1}\n",
        "y = np.where(y == 0, -1, 1)\n",
        "\n",
        "# Split into training and testing datasets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize SVM classifier\n",
        "clf = SVM(learning_rate=0.001, lambda_param=0.01, n_iters=1000)\n",
        "\n",
        "# Train the classifier\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "predictions = clf.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = np.mean(predictions == y_test)\n",
        "print(f\"SVM Classification accuracy on test set: {accuracy * 100:.2f}%\")\n",
        "```\n",
        "\n",
        "#### **Explanation:**\n",
        "\n",
        "1. **Loading the Dataset:**\n",
        "\n",
        "   We use `load_breast_cancer()` from `sklearn.datasets` to load the Breast Cancer dataset, which includes features and labels.\n",
        "\n",
        "   ```python\n",
        "   data = load_breast_cancer()\n",
        "   X, y = data.data, data.target\n",
        "   ```\n",
        "\n",
        "2. **Preprocessing:**\n",
        "\n",
        "   - **Feature Scaling:**\n",
        "\n",
        "     We standardize the features using `StandardScaler()` because SVMs are sensitive to the scale of the input features.\n",
        "\n",
        "     ```python\n",
        "     scaler = StandardScaler()\n",
        "     X = scaler.fit_transform(X)\n",
        "     ```\n",
        "\n",
        "   - **Label Conversion:**\n",
        "\n",
        "     Our SVM implementation expects labels in \\{-1, 1\\}, so we convert the labels accordingly.\n",
        "\n",
        "     ```python\n",
        "     y = np.where(y == 0, -1, 1)\n",
        "     ```\n",
        "\n",
        "3. **Splitting the Data:**\n",
        "\n",
        "   We split the dataset into training and testing sets using an 80-20 split.\n",
        "\n",
        "   ```python\n",
        "   from sklearn.model_selection import train_test_split\n",
        "   X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "   ```\n",
        "\n",
        "4. **Training the SVM Model:**\n",
        "\n",
        "   We initialize the SVM classifier with hyperparameters:\n",
        "\n",
        "   - `learning_rate=0.001`\n",
        "   - `lambda_param=0.01`\n",
        "   - `n_iters=1000`\n",
        "\n",
        "   Then, we train the model using the `fit()` method.\n",
        "\n",
        "   ```python\n",
        "   clf = SVM(learning_rate=0.001, lambda_param=0.01, n_iters=1000)\n",
        "   clf.fit(X_train, y_train)\n",
        "   ```\n",
        "\n",
        "   - **Gradient Descent Updates:**\n",
        "\n",
        "     During training, for each iteration and each sample, we check the condition \\( y_i (w^T x_i - b) \\geq 1 \\):\n",
        "\n",
        "     - **If the condition is true (correctly classified):**\n",
        "\n",
        "       Update the weights using the regularization term.\n",
        "\n",
        "       ```python\n",
        "       self.w -= self.learning_rate * (2 * self.lambda_param * self.w)\n",
        "       ```\n",
        "\n",
        "     - **If the condition is false (misclassified):**\n",
        "\n",
        "       Update the weights and bias using both the hinge loss and regularization term.\n",
        "\n",
        "       ```python\n",
        "       self.w -= self.learning_rate * (2 * self.lambda_param * self.w - np.dot(x_i, y_[idx]))\n",
        "       self.b -= self.learning_rate * y_[idx]\n",
        "       ```\n",
        "\n",
        "5. **Making Predictions and Evaluating the Model:**\n",
        "\n",
        "   We predict the labels for the test set and calculate the accuracy.\n",
        "\n",
        "   ```python\n",
        "   predictions = clf.predict(X_test)\n",
        "   accuracy = np.mean(predictions == y_test)\n",
        "   print(f\"SVM Classification accuracy on test set: {accuracy * 100:.2f}%\")\n",
        "   ```\n",
        "\n",
        "   - **Prediction Function:**\n",
        "\n",
        "     The `predict()` method computes the sign of the linear function \\( w^T x - b \\) for each sample.\n",
        "\n",
        "     ```python\n",
        "     def predict(self, X):\n",
        "         approx = np.dot(X, self.w) - self.b\n",
        "         return np.sign(approx)\n",
        "     ```\n",
        "\n",
        "#### **Results:**\n",
        "\n",
        "After running the code, you might get an output like:\n",
        "\n",
        "```\n",
        "SVM Classification accuracy on test set: 96.49%\n",
        "```\n",
        "\n",
        "**Note:** The actual accuracy may vary due to random initialization and data splitting.\n",
        "\n",
        "### **Visualization (Optional):**\n",
        "\n",
        "Since the dataset has 30 features, visualizing the decision boundary directly is not feasible. However, we can use **Principal Component Analysis (PCA)** to reduce the dimensionality to 2D for visualization purposes.\n",
        "\n",
        "```python\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Reduce to 2D using PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_projected = pca.fit_transform(X_test)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(X_projected[:, 0], X_projected[:, 1], c=predictions, cmap='coolwarm', alpha=0.8)\n",
        "plt.title('SVM Predictions (Projected onto 2D using PCA)')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "**Note:** This visualization is a projection and may not accurately reflect the true decision boundary in the original feature space.\n",
        "\n",
        "### **Adjusting Hyperparameters:**\n",
        "\n",
        "You can experiment with different hyperparameters to see how they affect the model's performance:\n",
        "\n",
        "- **Learning Rate (`learning_rate`):** A smaller learning rate may require more iterations but can lead to better convergence.\n",
        "- **Regularization Parameter (`lambda_param`):** Affects the trade-off between maximizing the margin and minimizing classification error.\n",
        "- **Number of Iterations (`n_iters`):** More iterations can improve accuracy up to a point but will increase computation time.\n",
        "\n",
        "### **Summary:**\n",
        "\n",
        "1. **Problem Formulation:**\n",
        "\n",
        "   - We started with the SVM optimization problem, converting it from a constrained problem to an unconstrained one using Lagrange multipliers.\n",
        "   - Introduced the hinge loss function to handle misclassifications.\n",
        "\n",
        "2. **Derivation of Update Rules:**\n",
        "\n",
        "   - Derived the gradients with respect to the weights and bias for both correctly classified and misclassified points.\n",
        "   - Developed update equations for gradient descent based on these gradients.\n",
        "\n",
        "3. **Implementation:**\n",
        "\n",
        "   - Implemented the SVM classifier in Python from scratch using NumPy.\n",
        "   - Applied the classifier to the Breast Cancer dataset and evaluated its performance.\n",
        "\n",
        "4. **Visualization:**\n",
        "\n",
        "   - Used PCA to project high-dimensional data into 2D for visualization of predictions.\n",
        "\n",
        "### **Conclusion:**\n",
        "\n",
        "We have successfully implemented an SVM classifier from scratch and applied it to a real-world dataset. This exercise demonstrates the practical application of SVMs and provides insight into how they work under the hood.\n",
        "\n",
        "### **Next Steps:**\n",
        "\n",
        "- **Kernel Trick:** Extend the implementation to handle non-linear data using kernel functions.\n",
        "- **Multi-class Classification:** Modify the algorithm to handle multi-class problems (e.g., using one-vs-rest approach).\n",
        "- **Optimization Techniques:** Implement advanced optimization methods like **SMO (Sequential Minimal Optimization)** for faster convergence.\n",
        "\n",
        "---\n",
        "\n",
        "Let me know if you have any questions or need further clarification on any part!"
      ],
      "metadata": {
        "id": "AeJ51HUQp2Ui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Convert labels from {0,1} to {-1,1}\n",
        "y = np.where(y == 0, -1, 1)\n",
        "\n",
        "# Split into training and testing datasets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize SVM classifier\n",
        "clf = SVM(learning_rate=0.001, lambda_param=0.01, n_iters=1000)\n",
        "\n",
        "# Train the classifier\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "predictions = clf.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = np.mean(predictions == y_test)\n",
        "print(f\"SVM Classification accuracy on test set: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "NHmVdsN_p1wW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Reduce to 2D using PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_projected = pca.fit_transform(X_test)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(X_projected[:, 0], X_projected[:, 1], c=predictions, cmap='coolwarm', alpha=0.8)\n",
        "plt.title('SVM Predictions (Projected onto 2D using PCA)')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Sorb6aRgqxxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To draw the decision boundary on the PCA-transformed 2D space, we first need to project the original data into 2D using PCA and then plot the SVM decision boundary. The decision boundary is defined by the equation \\( w^T x - b = 0 \\), where \\( w \\) and \\( b \\) are the weights and bias learned by the model.\n",
        "\n",
        "### Steps:\n",
        "\n",
        "1. **Reduce Data to 2D Using PCA:**\n",
        "   We will use PCA to transform the high-dimensional feature space (30 features) into 2D.\n",
        "   \n",
        "2. **Plot the Decision Boundary:**\n",
        "   The decision boundary will be a straight line in the 2D plane because of the linear nature of the SVM. We will compute the values for the line based on the projected weights and bias.\n",
        "\n",
        "3. **Visualize the Decision Boundary Along with the Data:**\n",
        "\n",
        "Let's implement this:\n",
        "\n",
        "### Code:\n",
        "\n",
        "```python\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Reduce the training and test set to 2D using PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "\n",
        "# SVM classifier from scratch (trained earlier)\n",
        "\n",
        "# Function to plot decision boundary\n",
        "def plot_decision_boundary(X, y, clf, pca, title=\"SVM Decision Boundary\"):\n",
        "    # Create a mesh to plot the decision boundary\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 500), np.linspace(y_min, y_max, 500))\n",
        "\n",
        "    # Project the mesh grid back to original space and predict using the classifier\n",
        "    grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
        "    grid_points_original_space = pca.inverse_transform(grid_points)\n",
        "    Z = clf.predict(grid_points_original_space)\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    # Plot decision boundary\n",
        "    plt.contourf(xx, yy, Z, alpha=0.8, cmap='coolwarm')\n",
        "    \n",
        "    # Plot the actual data points\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='k', marker='o', alpha=0.8)\n",
        "\n",
        "    # Plot support vectors (optional, not implemented in this SVM)\n",
        "    plt.title(title)\n",
        "    plt.xlabel('PCA Component 1')\n",
        "    plt.ylabel('PCA Component 2')\n",
        "    plt.show()\n",
        "\n",
        "# Plot decision boundary with PCA-transformed data\n",
        "plot_decision_boundary(X_test_pca, y_test, clf, pca, title=\"SVM Decision Boundary on PCA-transformed Data\")\n",
        "```\n",
        "\n",
        "### Explanation:\n",
        "\n",
        "1. **PCA Transformation:**\n",
        "   We first transform both the training and test data into 2D using PCA.\n",
        "   ```python\n",
        "   pca = PCA(n_components=2)\n",
        "   X_train_pca = pca.fit_transform(X_train)\n",
        "   X_test_pca = pca.transform(X_test)\n",
        "   ```\n",
        "\n",
        "2. **Mesh Grid:**\n",
        "   We create a mesh grid in the 2D PCA space, which will be used to evaluate the decision function at various points and plot the decision boundary.\n",
        "   ```python\n",
        "   x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "   y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "   xx, yy = np.meshgrid(np.linspace(x_min, x_max, 500), np.linspace(y_min, y_max, 500))\n",
        "   ```\n",
        "\n",
        "3. **Predicting for the Grid:**\n",
        "   We transform the grid points back into the original feature space and use our SVM classifier to predict the class at each grid point.\n",
        "   ```python\n",
        "   grid_points_original_space = pca.inverse_transform(grid_points)\n",
        "   Z = clf.predict(grid_points_original_space)\n",
        "   Z = Z.reshape(xx.shape)\n",
        "   ```\n",
        "\n",
        "4. **Plotting the Boundary:**\n",
        "   Finally, we use `plt.contourf()` to plot the decision boundary and the actual data points.\n",
        "   ```python\n",
        "   plt.contourf(xx, yy, Z, alpha=0.8, cmap='coolwarm')\n",
        "   plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='k', marker='o', alpha=0.8)\n",
        "   ```\n",
        "\n",
        "### Result:\n",
        "\n",
        "The plot will show the SVM decision boundary as a straight line in the 2D PCA space, separating the two classes (malignant and benign).\n",
        "\n",
        "Let me know if you'd like more clarification or improvements on the implementation!"
      ],
      "metadata": {
        "id": "K2mN3iU8rNeA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Reduce the training and test set to 2D using PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "\n",
        "# SVM classifier from scratch (trained earlier)\n",
        "\n",
        "# Function to plot decision boundary\n",
        "def plot_decision_boundary(X, y, clf, pca, title=\"SVM Decision Boundary\"):\n",
        "    # Create a mesh to plot the decision boundary\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 500), np.linspace(y_min, y_max, 500))\n",
        "\n",
        "    # Project the mesh grid back to original space and predict using the classifier\n",
        "    grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
        "    grid_points_original_space = pca.inverse_transform(grid_points)\n",
        "    Z = clf.predict(grid_points_original_space)\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    # Plot decision boundary\n",
        "    plt.contourf(xx, yy, Z, alpha=0.8, cmap='coolwarm')\n",
        "\n",
        "    # Plot the actual data points\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='k', marker='o', alpha=0.8)\n",
        "\n",
        "    # Plot support vectors (optional, not implemented in this SVM)\n",
        "    plt.title(title)\n",
        "    plt.xlabel('PCA Component 1')\n",
        "    plt.ylabel('PCA Component 2')\n",
        "    plt.show()\n",
        "\n",
        "# Plot decision boundary with PCA-transformed data\n",
        "plot_decision_boundary(X_test_pca, y_test, clf, pca, title=\"SVM Decision Boundary on PCA-transformed Data\")"
      ],
      "metadata": {
        "id": "amzvOodQrStu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}