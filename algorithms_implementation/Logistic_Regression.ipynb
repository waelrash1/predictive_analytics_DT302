{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNU327FkJEUJ0A3t1xTFA8d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/waelrash1/predictive_analytics_DT302/blob/main/algorithms_implementation/Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Linear Logistic Regression**\n",
        "\n",
        "step by step, from the mathematical derivation to the detailed Python implementation.\n",
        "\n",
        "### 1. **Introduction to Logistic Regression**\n",
        "\n",
        "Logistic regression is a statistical model that is commonly used for **binary classification** problems (where the target variable has two possible outcomes, e.g., 0 or 1). It models the probability that a given input belongs to a particular class.\n",
        "\n",
        "Unlike linear regression, which predicts continuous values, logistic regression predicts the probability of a class label being 1, based on the sigmoid function.\n",
        "\n",
        "### 2. **Logistic Function (Sigmoid Function)**\n",
        "\n",
        "The logistic regression model uses the **sigmoid function** to convert the linear combination of input features into a probability score between 0 and 1.\n",
        "\n",
        "The sigmoid function is given by:\n",
        "\n",
        "$\n",
        "h_\\theta(x) = \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$\n",
        "Where:\n",
        "- $ z = \\theta^T x = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... + \\theta_n x_n $\n",
        "- $ \\theta $ is the weight vector.\n",
        "- $ x $ is the input feature vector.\n",
        "\n",
        "The sigmoid function squashes any input value $ z $ into a range between 0 and 1, making it suitable for binary classification.\n",
        "\n",
        "### 3. **Logistic Regression Model**\n",
        "\n",
        "The hypothesis of logistic regression is given by the sigmoid of the linear model:\n",
        "\n",
        "$\n",
        "h_\\theta(x) = P(y=1 | x; \\theta) = \\frac{1}{1 + e^{-\\theta^T x}}\n",
        "$\n",
        "\n",
        "Where:\n",
        "- $ h_\\theta(x) $ represents the probability that the output $ y $ is 1 given input $ x $.\n",
        "\n",
        "For binary classification, the output $ y $ can be either 0 or 1, and the decision boundary is when $ P(y=1 | x; \\theta) = 0.5 $, or $ \\theta^T x = 0 $.\n",
        "\n",
        "### 4. **Loss Function (Cost Function)**\n",
        "\n",
        "The **cost function** for logistic regression is derived from the **likelihood function** (because we are modeling probabilities). The goal is to maximize the probability of correctly classifying the training data.\n",
        "\n",
        "The cost function $ J(\\theta) $ is the **negative log-likelihood**:\n",
        "\n",
        "$\n",
        "J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(h_\\theta(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_\\theta(x^{(i)})) \\right]\n",
        "$\n",
        "\n",
        "Where:\n",
        "- $ y^{(i)} $ is the true label (0 or 1) for the $i$-th example.\n",
        "- $ h_\\theta(x^{(i)}) $ is the predicted probability for the $i$-th example.\n",
        "- $ m $ is the number of training examples.\n",
        "\n",
        "This cost function penalizes incorrect predictions more heavily the further they are from the true label (0 or 1).\n",
        "\n",
        "### 5. **Gradient Descent for Logistic Regression**\n",
        "\n",
        "To minimize the cost function $ J(\\theta) $, we use **gradient descent**, an optimization algorithm that updates the weights iteratively to reduce the cost.\n",
        "\n",
        "The gradient of the cost function with respect to $ \\theta_j $ (for each feature $ j $) is given by:\n",
        "\n",
        "$\n",
        "\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^{m} \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right) x_j^{(i)}\n",
        "$\n",
        "\n",
        "We update the weights using gradient descent:\n",
        "\n",
        "$\n",
        "\\theta_j := \\theta_j - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_j}\n",
        "$\n",
        "\n",
        "Where:\n",
        "- $ \\alpha $ is the learning rate (a hyperparameter that controls the step size in gradient descent).\n",
        "\n",
        "### 6. **Logistic Regression Algorithm: Steps**\n",
        "\n",
        "1. **Initialize Parameters:**\n",
        "   Start with initial guesses for the weights $ \\theta = [\\theta_0, \\theta_1, ..., \\theta_n] $, typically initialized to zero or small random values.\n",
        "\n",
        "2. **Forward Pass:**\n",
        "   Compute the predicted probabilities $ h_\\theta(x) $ using the sigmoid function.\n",
        "\n",
        "3. **Cost Function:**\n",
        "   Calculate the cost function $ J(\\theta) $ using the current weights.\n",
        "\n",
        "4. **Gradient Descent:**\n",
        "   Update the weights $ \\theta $ using the gradient descent rule.\n",
        "\n",
        "5. **Repeat:**\n",
        "   Iterate steps 2 to 4 until the cost converges (i.e., until it becomes very small or the difference between successive updates is minimal).\n",
        "\n",
        "### 7. **Python Implementation**\n",
        "\n",
        "Let's implement logistic regression from scratch using NumPy.\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "class LogisticRegression:\n",
        "    def __init__(self, learning_rate=0.01, n_iters=1000):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iters = n_iters\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Number of training examples and features\n",
        "        n_samples, n_features = X.shape\n",
        "        \n",
        "        # Initialize parameters (weights and bias)\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "        \n",
        "        # Gradient descent\n",
        "        for _ in range(self.n_iters):\n",
        "            # Linear model (z = X * w + b)\n",
        "            linear_model = np.dot(X, self.weights) + self.bias\n",
        "            \n",
        "            # Apply sigmoid function to get probabilities (h_theta(x))\n",
        "            y_predicted = self.sigmoid(linear_model)\n",
        "            \n",
        "            # Compute gradients\n",
        "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
        "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
        "            \n",
        "            # Update weights and bias\n",
        "            self.weights -= self.learning_rate * dw\n",
        "            self.bias -= self.learning_rate * db\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Linear model (z = X * w + b)\n",
        "        linear_model = np.dot(X, self.weights) + self.bias\n",
        "        # Apply sigmoid function\n",
        "        y_predicted = self.sigmoid(linear_model)\n",
        "        # Convert probabilities to binary output (0 or 1)\n",
        "        y_predicted_cls = [1 if i > 0.5 else 0 for i in y_predicted]\n",
        "        return np.array(y_predicted_cls)\n",
        "\n",
        "# Example usage\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load a dataset\n",
        "bc = datasets.load_breast_cancer()\n",
        "X, y = bc.data, bc.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the logistic regression model\n",
        "model = LogisticRegression(learning_rate=0.01, n_iters=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = np.mean(predictions == y_test)\n",
        "print(f\"Logistic Regression accuracy: {accuracy * 100:.2f}%\")\n",
        "```\n",
        "\n",
        "### Explanation of the Code:\n",
        "\n",
        "1. **Initialization:**\n",
        "   The `LogisticRegression` class has two important hyperparameters:\n",
        "   - `learning_rate`: Controls the size of the steps in the gradient descent.\n",
        "   - `n_iters`: Number of iterations for the gradient descent optimization.\n",
        "\n",
        "   The `fit` method is responsible for training the model, and the `predict` method is used to make predictions on new data.\n",
        "\n",
        "2. **Sigmoid Function:**\n",
        "   The `sigmoid` function is used to convert the linear model's output into probabilities.\n",
        "   \n",
        "   ```python\n",
        "   def sigmoid(self, z):\n",
        "       return 1 / (1 + np.exp(-z))\n",
        "   ```\n",
        "\n",
        "3. **Gradient Descent:**\n",
        "   The `fit` method computes the linear model output, applies the sigmoid function, and then uses the gradients of the cost function to update the weights and bias.\n",
        "\n",
        "   The gradient descent step looks like this:\n",
        "   \n",
        "   ```python\n",
        "   dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
        "   db = (1 / n_samples) * np.sum(y_predicted - y)\n",
        "   \n",
        "   self.weights -= self.learning_rate * dw\n",
        "   self.bias -= self.learning_rate * db\n",
        "   ```\n",
        "\n",
        "4. **Prediction:**\n",
        "   In the `predict` method, the model outputs probabilities, and then we threshold these probabilities at 0.5 to get class predictions.\n",
        "\n",
        "   ```python\n",
        "   y_predicted_cls = [1 if i > 0.5 else 0 for i in y_predicted]\n",
        "   ```\n",
        "\n",
        "### 8. **Example Output:**\n",
        "\n",
        "After running the code on the breast cancer dataset, you may get output like:\n",
        "\n",
        "```\n",
        "Logistic Regression accuracy: 95.61%\n",
        "```\n",
        "\n",
        "### 9. **Evaluation and Improvements:**\n",
        "\n",
        "- **Accuracy:** Logistic regression is often a good first model to try for binary classification problems due to its simplicity and interpretability.\n",
        "- **Regularization:** You can add L1 (lasso) or L2 (ridge) regularization to the cost function to avoid overfitting on high-dimensional datasets.\n",
        "- **Hyperparameter Tuning:** Experimenting with learning rates and the number of iterations can improve model performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "VAI6awDkz-ip"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHexjFFCz9az"
      },
      "outputs": [],
      "source": []
    }
  ]
}