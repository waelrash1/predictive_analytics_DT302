{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNlL1bdI/Te36y0GYAnH4Fw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/waelrash1/predictive_analytics_DT302/blob/main/gradient_decent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjeISqbNu7i_"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from matplotlib import cm\n",
        "\n",
        "# Define the function\n",
        "def f(x, y):\n",
        "  #return x**2 + y**2-x*y*np.sin(x*y)\n",
        "  return x**2 + y**2\n",
        "\n",
        "# Define the gradient of the function\n",
        "def gradient(x, y):\n",
        "  return np.array([2*x, 2*y])\n",
        "\n",
        "# Initialize starting point\n",
        "x = 5\n",
        "y = 5\n",
        "learning_rate = 0.1\n",
        "num_iterations = 20\n",
        "\n",
        "# Store the trajectory of the gradient descent\n",
        "x_history = [x]\n",
        "y_history = [y]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a meshgrid for visualization\n",
        "x_range = np.linspace(-6, 6, 100)\n",
        "y_range = np.linspace(-6, 6, 100)\n",
        "X, Y = np.meshgrid(x_range, y_range)\n",
        "Z = f(X, Y)\n",
        "\n"
      ],
      "metadata": {
        "id": "ok88Byj_-0X4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the function in 3D\n",
        "fig = plt.figure(figsize=(20, 12))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.plot_surface(X, Y, Z, cmap=cm.viridis)\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "ax.set_zlabel('f(x, y)')\n",
        "ax.set_title('Function Visualization')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hwfBzZvH-37N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform gradient descent\n",
        "for i in range(num_iterations):\n",
        "  grad = gradient(x, y)\n",
        "  x = x - learning_rate * grad[0]\n",
        "  y = y - learning_rate * grad[1]\n",
        "  x_history.append(x)\n",
        "  y_history.append(y)\n"
      ],
      "metadata": {
        "id": "CXJ_luca-tB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the contour\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.contour(X, Y, Z, levels=20)\n",
        "plt.plot(x_history, y_history, marker='o', linestyle='-', color='red')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.title('Gradient Descent Trajectory')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lV0MdgtK-2_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Plot the actual function values along the trajectory\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(len(x_history)), [f(x, y) for x, y in zip(x_history, y_history)], marker='o', linestyle='-', color='blue')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('f(x, y)')\n",
        "plt.title('Function Values along Trajectory')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LRQjtCXN-7od"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# Plot the derivative (gradient) values along the trajectory\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(len(x_history)), [np.linalg.norm(gradient(x,y)) for x, y in zip(x_history, y_history)], marker='o', linestyle='-', color='green')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Gradient Norm')\n",
        "plt.title('Gradient Norm along Trajectory')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SZ7v9H2U--Ld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stochastic Gradient desent\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from matplotlib import cm\n",
        "\n",
        "# Define the function\n",
        "def f(x, y):\n",
        "  return x**2 + y**2\n",
        "\n",
        "# Define the gradient of the function\n",
        "def gradient(x, y):\n",
        "  return np.array([2*x, 2*y])\n",
        "\n",
        "# Initialize starting point\n",
        "x = 5\n",
        "y = 5\n",
        "learning_rate = 0.1\n",
        "num_iterations = 20\n",
        "batch_size = 1  # Stochastic gradient descent\n",
        "\n",
        "# Store the trajectory of the gradient descent\n",
        "x_history = [x]\n",
        "y_history = [y]\n",
        "\n",
        "# Perform stochastic gradient descent\n",
        "for i in range(num_iterations):\n",
        "  # Generate a random sample (in this case, just one data point)\n",
        "  # Typically, this would be a subset of your actual data.\n",
        "  # For simplicity, we're just using the current (x, y) point.\n",
        "  x_sample = x\n",
        "  y_sample = y\n",
        "\n",
        "  grad = gradient(x_sample, y_sample)\n",
        "  x = x - learning_rate * grad[0]\n",
        "  y = y - learning_rate * grad[1]\n",
        "  x_history.append(x)\n",
        "  y_history.append(y)\n",
        "\n",
        "# Create a meshgrid for visualization\n",
        "x_range = np.linspace(-6, 6, 100)\n",
        "y_range = np.linspace(-6, 6, 100)\n",
        "X, Y = np.meshgrid(x_range, y_range)\n",
        "Z = f(X, Y)\n",
        "\n",
        "# Plot the contour\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.contour(X, Y, Z, levels=20)\n",
        "plt.plot(x_history, y_history, marker='o', linestyle='-', color='red')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.title('Stochastic Gradient Descent Trajectory')\n",
        "plt.show()\n",
        "\n",
        "# Plot the function in 3D\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.plot_surface(X, Y, Z, cmap=cm.viridis)\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "ax.set_zlabel('f(x, y)')\n",
        "ax.set_title('Function Visualization')\n",
        "plt.show()\n",
        "\n",
        "# Plot the actual function values along the trajectory\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(len(x_history)), [f(x, y) for x, y in zip(x_history, y_history)], marker='o', linestyle='-', color='blue')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('f(x, y)')\n",
        "plt.title('Function Values along Trajectory')\n",
        "plt.show()\n",
        "\n",
        "# Plot the derivative (gradient) values along the trajectory\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(len(x_history)), [np.linalg.norm(gradient(x,y)) for x, y in zip(x_history, y_history)], marker='o', linestyle='-', color='green')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Gradient Norm')\n",
        "plt.title('Gradient Norm along Trajectory')\n",
        "plt.show()\n",
        "\n",
        "# Additional visualization:\n",
        "# 1. Animate the gradient descent process (using FuncAnimation) to show the step-by-step progress.\n",
        "# 2. Create a scatter plot of the data points (if you're using real data) along with the contour plot to see how the gradient descent is fitting the data.\n",
        "# 3. Compare the convergence speed of SGD with batch gradient descent and mini-batch gradient descent.\n"
      ],
      "metadata": {
        "id": "KLSZh8JJvak4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}